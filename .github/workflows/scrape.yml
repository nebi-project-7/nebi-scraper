name: Weekly Container Price Scraper

on:
  schedule:
    - cron: '0 6 * * 0'  # Sonntags 6 Uhr
  workflow_dispatch:
    inputs:
      spiders:
        description: 'Spider zum Ausf√ºhren (komma-getrennt oder "all")'
        required: true
        default: 'all'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: üåê Setup Chrome
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      - name: üì¶ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: üï∑Ô∏è Run Spiders
        run: |
          cd nebi_spiders
          mkdir -p ../data ../logs

          # Hilfsfunktion: Spider-Name aus Python-Datei extrahieren
          get_spider_name() {
            grep -oP "^\s*name\s*=\s*['\"]\\K[^'\"]+" "$1" | head -1
          }

          # Alle Stadt-Ordner finden
          CITIES=$(ls -d spiders/*/ 2>/dev/null | xargs -n1 basename | grep -v __pycache__ | tr '\n' ' ')
          INPUT="${{ github.event.inputs.spiders }}"
          INPUT_LOWER=$(echo "$INPUT" | tr '[:upper:]' '[:lower:]')

          echo "üìç Verf√ºgbare St√§dte: $CITIES"
          echo "üìã Input: $INPUT (lowercase: $INPUT_LOWER)"

          # Spider-Namen sammeln
          SPIDERS_TO_RUN=""

          # Wenn Input leer oder "all" ‚Üí alle Spider via scrapy list
          if [ -z "$INPUT" ] || [ "$INPUT_LOWER" = "all" ]; then
            echo "üåç Running ALL spiders"
            rm -f ../data/*.json ../logs/*.txt
            echo "üóëÔ∏è Deleted all JSON and log files"
            SPIDERS_TO_RUN=$(scrapy list)

          # Wenn Input eine Stadt ist
          elif echo "$CITIES" | grep -qw "$INPUT_LOWER"; then
            echo "üèôÔ∏è Running all spiders from: $INPUT_LOWER"
            for spider_file in spiders/$INPUT_LOWER/*.py; do
              if [ "$(basename $spider_file)" != "__init__.py" ]; then
                spider_name=$(get_spider_name "$spider_file")
                if [ -n "$spider_name" ]; then
                  rm -f "../data/${spider_name}-products.json" "../logs/${spider_name}.txt"
                  SPIDERS_TO_RUN="$SPIDERS_TO_RUN $spider_name"
                fi
              fi
            done

          # Wenn Input mehrere Werte (St√§dte oder Spider)
          elif echo "$INPUT" | grep -q ","; then
            IFS=',' read -ra ITEMS <<< "$INPUT"
            for item in "${ITEMS[@]}"; do
              item=$(echo "$item" | xargs)
              item_lower=$(echo "$item" | tr '[:upper:]' '[:lower:]')
              if echo "$CITIES" | grep -qw "$item_lower"; then
                echo "üèôÔ∏è Adding city: $item_lower"
                for spider_file in spiders/$item_lower/*.py; do
                  if [ "$(basename $spider_file)" != "__init__.py" ]; then
                    spider_name=$(get_spider_name "$spider_file")
                    if [ -n "$spider_name" ]; then
                      rm -f "../data/${spider_name}-products.json" "../logs/${spider_name}.txt"
                      SPIDERS_TO_RUN="$SPIDERS_TO_RUN $spider_name"
                    fi
                  fi
                done
              else
                echo "üï∑Ô∏è Adding spider: $item"
                rm -f "../data/${item}-products.json" "../logs/${item}.txt"
                SPIDERS_TO_RUN="$SPIDERS_TO_RUN $item"
              fi
            done

          # Sonst: Input sind Spider-Namen
          else
            rm -f "../data/${INPUT}-products.json" "../logs/${INPUT}.txt"
            SPIDERS_TO_RUN="$INPUT"
            echo "üï∑Ô∏è Running specific spider: $INPUT"
          fi

          echo "=========================================="
          echo "üï∑Ô∏è Spiders to run: $SPIDERS_TO_RUN"
          echo "=========================================="

          # Jeden Spider ausf√ºhren
          for spider in $SPIDERS_TO_RUN; do
            spider=$(echo "$spider" | xargs)
            if [ -n "$spider" ]; then
              echo "=========================================="
              echo "üï∑Ô∏è Starting: $spider"
              echo "=========================================="
              scrapy crawl "$spider" -O "../data/${spider}-products.json" 2>&1 | tee "../logs/${spider}.txt" || true
              echo "‚úÖ Finished: $spider"
            fi
          done

          echo "‚úÖ All selected spiders completed!"

      - name: üìä Create Combined JSON
        if: github.event.inputs.spiders == 'all' || github.event.inputs.spiders == ''
        run: |
          python << 'EOF'
          import json
          import glob
          from datetime import datetime

          all_products = []
          sources = {}

          for file in glob.glob('data/*.json'):
              if 'all_products' in file:
                  continue
              try:
                  with open(file, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if not content:
                          continue
                      products = json.loads(content)
                      if isinstance(products, list):
                          all_products.extend(products)
                          source_name = file.replace('data/', '').replace('.json', '')
                          sources[source_name] = len(products)
                          print(f"‚úÖ {source_name}: {len(products)} Produkte")
              except Exception as e:
                  print(f"‚ö†Ô∏è Fehler bei {file}: {e}")

          output = {
              "last_updated": datetime.utcnow().isoformat() + "Z",
              "total_products": len(all_products),
              "sources": sources,
              "products": all_products
          }

          with open('data/all_products.json', 'w', encoding='utf-8') as f:
              json.dump(output, f, ensure_ascii=False, indent=2)

          print(f"üì¶ GESAMT: {len(all_products)} Produkte")
          EOF

      - name: üìÖ Create Monthly Archive
        if: github.event.inputs.spiders == 'all' || github.event.inputs.spiders == ''
        run: |
          # Aktueller Monat (z.B. 2025-12)
          CURRENT_MONTH=$(date +'%Y-%m')
          ARCHIVE_DIR="data/archive/${CURRENT_MONTH}"

          # Pr√ºfe ob Archiv f√ºr diesen Monat bereits existiert
          if [ ! -d "$ARCHIVE_DIR" ]; then
            echo "üìÖ Erstelle monatliches Archiv f√ºr: $CURRENT_MONTH"
            mkdir -p "$ARCHIVE_DIR"

            # Kopiere alle JSON-Dateien ins Archiv
            for json_file in data/*.json; do
              if [ -f "$json_file" ]; then
                filename=$(basename "$json_file")
                cp "$json_file" "$ARCHIVE_DIR/$filename"
                echo "  ‚úì $filename"
              fi
            done

            echo "‚úÖ Archiv erstellt: $ARCHIVE_DIR"
          else
            echo "üìÖ Archiv f√ºr $CURRENT_MONTH existiert bereits - √ºberspringe"
          fi

      - name: üì§ Commit & Push Results
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add .
          git diff --staged --quiet || git commit -m "üï∑Ô∏è Scraper update: ${{ github.event.inputs.spiders || 'all' }} - $(date +'%Y-%m-%d %H:%M')"
          git pull --rebase origin main || true
          git push

      - name: üìÅ Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results-${{ github.run_number }}
          path: |
            data/*.json
            logs/*.txt
          retention-days: 30
