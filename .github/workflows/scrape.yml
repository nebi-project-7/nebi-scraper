name: Weekly Container Price Scraper

on:
  schedule:
    - cron: '0 6 * * 0'  # Sonntags 6 Uhr
  workflow_dispatch:
    inputs:
      spiders:
        description: 'Spider zum AusfÃ¼hren (komma-getrennt oder "all")'
        required: true
        default: 'all'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸŒ Setup Chrome
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      - name: ğŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: ğŸ—‘ï¸ Clean Old Data
        run: |
          mkdir -p data logs
          if [ "${{ github.event.inputs.spiders }}" = "all" ] || [ -z "${{ github.event.inputs.spiders }}" ]; then
            rm -f data/*.json
            rm -f logs/*.txt
            echo "ğŸ—‘ï¸ Deleted all JSON and log files"
          else
            IFS=',' read -ra SPIDER_ARRAY <<< "${{ github.event.inputs.spiders }}"
            for spider in "${SPIDER_ARRAY[@]}"; do
              spider=$(echo "$spider" | xargs)
              rm -f "data/${spider}.json"
              rm -f "logs/${spider}.txt"
              echo "ğŸ—‘ï¸ Deleted ${spider}.json and ${spider}.txt"
            done
          fi

      - name: ğŸ•·ï¸ Run Spiders
        run: |
          # Liste aller verfÃ¼gbaren Spider (komma-getrennt)
          ALL_SPIDERS="abc-container,albaclick,aser-container-products,berlin-recycling,cdz-berlin,containerfritze,dare-shop-products,elno-container-products,kati-bau-container,klebs-container-products,kroll-container,ores-container-products,schuttgeier-products,todra-container-products,ts-container-products"

          # Bestimme welche Spider laufen sollen
          if [ "${{ github.event.inputs.spiders }}" = "all" ] || [ -z "${{ github.event.inputs.spiders }}" ]; then
            SPIDERS_TO_RUN="$ALL_SPIDERS"
            echo "ğŸ•·ï¸ Running ALL spiders"
          else
            SPIDERS_TO_RUN="${{ github.event.inputs.spiders }}"
            echo "ğŸ•·ï¸ Running selected spiders: $SPIDERS_TO_RUN"
          fi

          # FÃ¼hre jeden Spider aus
          IFS=',' read -ra SPIDER_ARRAY <<< "$SPIDERS_TO_RUN"
          for spider in "${SPIDER_ARRAY[@]}"; do
            spider=$(echo "$spider" | xargs)  # Trim whitespace
            echo "=========================================="
            echo "ğŸ•·ï¸ Starting spider: $spider"
            echo "=========================================="
            scrapy crawl "$spider" -o "data/${spider}.json" --logfile "logs/${spider}.txt" 2>&1 || echo "âš ï¸ $spider failed"
            echo "âœ… Finished: $spider"
          done

          echo "âœ… All selected spiders completed!"

      - name: ğŸ“Š Create Combined JSON
        if: github.event.inputs.spiders == 'all' || github.event.inputs.spiders == ''
        run: |
          python << 'EOF'
          import json
          import glob
          from datetime import datetime

          all_products = []
          sources = {}

          for file in glob.glob('data/*.json'):
              if 'all_products' in file:
                  continue
              try:
                  with open(file, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if not content:
                          continue
                      products = json.loads(content)
                      if isinstance(products, list):
                          all_products.extend(products)
                          source_name = file.replace('data/', '').replace('.json', '')
                          sources[source_name] = len(products)
                          print(f"âœ… {source_name}: {len(products)} Produkte")
              except Exception as e:
                  print(f"âš ï¸ Fehler bei {file}: {e}")

          output = {
              "last_updated": datetime.utcnow().isoformat() + "Z",
              "total_products": len(all_products),
              "sources": sources,
              "products": all_products
          }

          with open('data/all_products.json', 'w', encoding='utf-8') as f:
              json.dump(output, f, ensure_ascii=False, indent=2)

          print(f"ğŸ“¦ GESAMT: {len(all_products)} Produkte")
          EOF

      - name: ğŸ“¤ Commit & Push Results
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add .
          git diff --staged --quiet || git commit -m "ğŸ•·ï¸ Scraper update: ${{ github.event.inputs.spiders || 'all' }} - $(date +'%Y-%m-%d %H:%M')"
          git pull --rebase origin main
          git push

      - name: ğŸ“ Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results-${{ github.run_number }}
          path: |
            data/*.json
            logs/*.txt
          retention-days: 30
