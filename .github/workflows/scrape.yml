name: Weekly Container Price Scraper

on:
  schedule:
    - cron: '0 6 * * 0'  # Sonntags 6 Uhr
  workflow_dispatch:
    inputs:
      spiders:
        description: 'Spider zum AusfÃ¼hren (komma-getrennt oder "all")'
        required: true
        default: 'all'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸŒ Setup Chrome
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      - name: ğŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: ğŸ—‘ï¸ Clean Old Data
        run: |
          mkdir -p data logs
          cd nebi_spiders
          CITIES=$(ls -d spiders/*/ 2>/dev/null | xargs -n1 basename | grep -v __pycache__ | tr '\n' ' ')
          INPUT="${{ github.event.inputs.spiders }}"
          INPUT_LOWER=$(echo "$INPUT" | tr '[:upper:]' '[:lower:]')

          if [ -z "$INPUT" ] || [ "$INPUT_LOWER" = "all" ]; then
            rm -f ../data/*.json
            rm -f ../logs/*.txt
            echo "ğŸ—‘ï¸ Deleted all JSON and log files"
          elif echo "$CITIES" | grep -qw "$INPUT_LOWER"; then
            for spider_file in spiders/$INPUT_LOWER/*.py; do
              if [ "$(basename $spider_file)" != "__init__.py" ]; then
                spider_name=$(basename $spider_file .py)
                rm -f "../data/${spider_name}-products.json"
                rm -f "../logs/${spider_name}.txt"
                echo "ğŸ—‘ï¸ Deleted ${spider_name} files"
              fi
            done
          else
            IFS=',' read -ra ITEMS <<< "$INPUT"
            for item in "${ITEMS[@]}"; do
              item=$(echo "$item" | xargs)
              item_lower=$(echo "$item" | tr '[:upper:]' '[:lower:]')
              if echo "$CITIES" | grep -qw "$item_lower"; then
                for spider_file in spiders/$item_lower/*.py; do
                  if [ "$(basename $spider_file)" != "__init__.py" ]; then
                    spider_name=$(basename $spider_file .py)
                    rm -f "../data/${spider_name}-products.json"
                    rm -f "../logs/${spider_name}.txt"
                  fi
                done
              else
                rm -f "../data/${item}-products.json"
                rm -f "../logs/${item}.txt"
                echo "ğŸ—‘ï¸ Deleted ${item} files"
              fi
            done
          fi

      - name: ğŸ•·ï¸ Run Spiders
        run: |
          cd nebi_spiders

          # Automatisch alle Stadt-Ordner finden
          CITIES=$(ls -d spiders/*/ 2>/dev/null | xargs -n1 basename | grep -v __pycache__ | tr '\n' ' ')
          INPUT="${{ github.event.inputs.spiders }}"
          # Input zu Kleinbuchstaben fÃ¼r case-insensitiven Vergleich
          INPUT_LOWER=$(echo "$INPUT" | tr '[:upper:]' '[:lower:]')

          echo "ğŸ“ VerfÃ¼gbare StÃ¤dte: $CITIES"
          echo "ğŸ“‹ Input: $INPUT (lowercase: $INPUT_LOWER)"

          # Wenn Input leer oder "all" â†’ alle Spider
          if [ -z "$INPUT" ] || [ "$INPUT_LOWER" = "all" ]; then
            SPIDERS_TO_RUN=$(find spiders/*/ -name "*.py" ! -name "__init__.py" -exec basename {} .py \; 2>/dev/null)
            echo "ğŸŒ Running ALL spiders"

          # Wenn Input eine Stadt ist (case-insensitiv)
          elif echo "$CITIES" | grep -qw "$INPUT_LOWER"; then
            SPIDERS_TO_RUN=$(find spiders/$INPUT_LOWER/ -name "*.py" ! -name "__init__.py" -exec basename {} .py \; 2>/dev/null)
            echo "ğŸ™ï¸ Running all spiders from: $INPUT_LOWER"

          # Wenn Input mehrere Werte (StÃ¤dte oder Spider)
          elif echo "$INPUT" | grep -q ","; then
            SPIDERS_TO_RUN=""
            IFS=',' read -ra ITEMS <<< "$INPUT"
            for item in "${ITEMS[@]}"; do
              item=$(echo "$item" | xargs)
              item_lower=$(echo "$item" | tr '[:upper:]' '[:lower:]')
              if echo "$CITIES" | grep -qw "$item_lower"; then
                echo "ğŸ™ï¸ Adding city: $item_lower"
                SPIDERS_TO_RUN="$SPIDERS_TO_RUN $(find spiders/$item_lower/ -name "*.py" ! -name "__init__.py" -exec basename {} .py \; 2>/dev/null)"
              else
                echo "ğŸ•·ï¸ Adding spider: $item"
                SPIDERS_TO_RUN="$SPIDERS_TO_RUN $item"
              fi
            done

          # Sonst: Input sind Spider-Namen
          else
            SPIDERS_TO_RUN="$INPUT"
            echo "ğŸ•·ï¸ Running specific spiders: $INPUT"
          fi

          echo "=========================================="
          echo "ğŸ•·ï¸ Spiders to run: $SPIDERS_TO_RUN"
          echo "=========================================="

          # Jeden Spider ausfÃ¼hren
          for spider in $SPIDERS_TO_RUN; do
            spider=$(echo "$spider" | xargs)
            if [ -n "$spider" ]; then
              echo "=========================================="
              echo "ğŸ•·ï¸ Starting: $spider"
              echo "=========================================="
              scrapy crawl "$spider" -O "../data/${spider}-products.json" 2>&1 | tee "../logs/${spider}.txt" || true
              echo "âœ… Finished: $spider"
            fi
          done

          echo "âœ… All selected spiders completed!"

      - name: ğŸ“Š Create Combined JSON
        if: github.event.inputs.spiders == 'all' || github.event.inputs.spiders == ''
        run: |
          python << 'EOF'
          import json
          import glob
          from datetime import datetime

          all_products = []
          sources = {}

          for file in glob.glob('data/*.json'):
              if 'all_products' in file:
                  continue
              try:
                  with open(file, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if not content:
                          continue
                      products = json.loads(content)
                      if isinstance(products, list):
                          all_products.extend(products)
                          source_name = file.replace('data/', '').replace('.json', '')
                          sources[source_name] = len(products)
                          print(f"âœ… {source_name}: {len(products)} Produkte")
              except Exception as e:
                  print(f"âš ï¸ Fehler bei {file}: {e}")

          output = {
              "last_updated": datetime.utcnow().isoformat() + "Z",
              "total_products": len(all_products),
              "sources": sources,
              "products": all_products
          }

          with open('data/all_products.json', 'w', encoding='utf-8') as f:
              json.dump(output, f, ensure_ascii=False, indent=2)

          print(f"ğŸ“¦ GESAMT: {len(all_products)} Produkte")
          EOF

      - name: ğŸ“¤ Commit & Push Results
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add .
          git diff --staged --quiet || git commit -m "ğŸ•·ï¸ Scraper update: ${{ github.event.inputs.spiders || 'all' }} - $(date +'%Y-%m-%d %H:%M')"
          git pull --rebase origin main || true
          git push

      - name: ğŸ“ Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results-${{ github.run_number }}
          path: |
            data/*.json
            logs/*.txt
          retention-days: 30
