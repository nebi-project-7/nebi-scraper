name: Weekly Container Price Scraper

on:
  schedule:
    - cron: '0 7 * * 1'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ğŸŒ Setup Chrome
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      - name: ğŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸ•·ï¸ Run All Scrapy Spiders
        run: |
          mkdir -p data logs
          
          echo "ğŸ•·ï¸ Starting all spiders..."
          
          scrapy crawl abc-container -o data/abc-container.json --logfile logs/abc-container.txt 2>&1 || echo "âš ï¸ abc-container failed"
          scrapy crawl albaclick -o data/albaclick.json --logfile logs/albaclick.txt 2>&1 || echo "âš ï¸ albaclick failed"
          scrapy crawl aser-container-products -o data/aser-container-products.json --logfile logs/aser-container-products.txt 2>&1 || echo "âš ï¸ aser-container-products failed"
          scrapy crawl berlin-recycling -o data/berlin-recycling.json --logfile logs/berlin-recycling.txt 2>&1 || echo "âš ï¸ berlin-recycling failed"
          scrapy crawl cdz-berlin -o data/cdz-berlin.json --logfile logs/cdz-berlin.txt 2>&1 || echo "âš ï¸ cdz-berlin failed"
          scrapy crawl containerfritze -o data/containerfritze.json --logfile logs/containerfritze.txt 2>&1 || echo "âš ï¸ containerfritze failed"
          scrapy crawl dare-shop-products -o data/dare-shop-products.json --logfile logs/dare-shop-products.txt 2>&1 || echo "âš ï¸ dare-shop-products failed"
          scrapy crawl elno-container-products -o data/elno-container-products.json --logfile logs/elno-container-products.txt 2>&1 || echo "âš ï¸ elno-container-products failed"
          scrapy crawl kati-bau-container -o data/kati-bau-container.json --logfile logs/kati-bau-container.txt 2>&1 || echo "âš ï¸ kati-bau-container failed"
          scrapy crawl klebs-container-products -o data/klebs-container-products.json --logfile logs/klebs-container-products.txt 2>&1 || echo "âš ï¸ klebs-container-products failed"
          scrapy crawl kroll-container -o data/kroll-container.json --logfile logs/kroll-container.txt 2>&1 || echo "âš ï¸ kroll-container failed"
          scrapy crawl ores-container-products -o data/ores-container-products.json --logfile logs/ores-container-products.txt 2>&1 || echo "âš ï¸ ores-container-products failed"
          scrapy crawl schuttgeier-products -o data/schuttgeier-products.json --logfile logs/schuttgeier-products.txt 2>&1 || echo "âš ï¸ schuttgeier-products failed"
          scrapy crawl todra-container-products -o data/todra-container-products.json --logfile logs/todra-container-products.txt 2>&1 || echo "âš ï¸ todra-container-products failed"
          scrapy crawl ts-container-products -o data/ts-container-products.json --logfile logs/ts-container-products.txt 2>&1 || echo "âš ï¸ ts-container-products failed"
          
          echo "âœ… All spiders completed!"
      
      - name: ğŸ“Š Create Combined JSON
        run: |
          python << 'EOF'
          import json
          import glob
          from datetime import datetime
          
          all_products = []
          sources = {}
          
          for file in glob.glob('data/*.json'):
              if 'all_products' in file:
                  continue
              try:
                  with open(file, 'r', encoding='utf-8') as f:
                      content = f.read().strip()
                      if not content:
                          continue
                      products = json.loads(content)
                      if isinstance(products, list):
                          all_products.extend(products)
                          source_name = file.replace('data/', '').replace('.json', '')
                          sources[source_name] = len(products)
                          print(f"âœ… {source_name}: {len(products)} Produkte")
              except Exception as e:
                  print(f"âš ï¸ Fehler bei {file}: {e}")
          
          output = {
              "last_updated": datetime.utcnow().isoformat() + "Z",
              "total_products": len(all_products),
              "sources": sources,
              "products": all_products
          }
          
          with open('data/all_products.json', 'w', encoding='utf-8') as f:
              json.dump(output, f, ensure_ascii=False, indent=2)
          
          print(f"ğŸ“¦ GESAMT: {len(all_products)} Produkte")
          EOF
      
      - name: ğŸ“¤ Commit & Push Results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/*.json logs/*.txt
          if git diff --staged --quiet; then
            echo "Keine Ã„nderungen"
          else
            git commit -m "ğŸ”„ Scraping Update $(date +'%Y-%m-%d %H:%M')"
            git pull --rebase
            git push
          fi
      
      - name: ğŸ“ Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scraping-results-${{ github.run_number }}
          path: |
            data/*.json
            logs/*.txt
          retention-days: 30
