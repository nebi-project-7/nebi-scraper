2025-12-11 21:11:25 [scrapy.utils.log] INFO: Scrapy 2.13.4 started (bot: nebi_spiders)
2025-12-11 21:11:25 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.2',
 'libxml2': '2.14.6',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.11.14 (main, Oct 10 2025, 01:03:14) [GCC 13.3.0]',
 'pyOpenSSL': '25.3.0 (OpenSSL 3.5.4 30 Sep 2025)',
 'cryptography': '46.0.3',
 'Platform': 'Linux-6.11.0-1018-azure-x86_64-with-glibc2.39'}
2025-12-11 21:11:25 [selenium.webdriver.common.selenium_manager] DEBUG: Selenium Manager binary found at: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager
2025-12-11 21:11:25 [selenium.webdriver.common.selenium_manager] DEBUG: Executing process: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome --language-binding python --output json
2025-12-11 21:11:25 [selenium.webdriver.common.selenium_manager] DEBUG: Driver path: /usr/bin/chromedriver
2025-12-11 21:11:25 [selenium.webdriver.common.selenium_manager] DEBUG: Browser path: /usr/bin/google-chrome
2025-12-11 21:11:25 [selenium.webdriver.common.service] DEBUG: Started executable: `/usr/bin/chromedriver` in a child process with pid: 2131 using 0 to output -3
2025-12-11 21:11:25 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://localhost:33785/session {'capabilities': {'firstMatch': [{}], 'alwaysMatch': {'browserName': 'chrome', 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>, 'browserVersion': None, 'goog:chromeOptions': {'extensions': [], 'binary': '/usr/bin/google-chrome', 'args': []}}}}
2025-12-11 21:11:25 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:33785
2025-12-11 21:11:26 [urllib3.connectionpool] DEBUG: http://localhost:33785 "POST /session HTTP/1.1" 500 0
2025-12-11 21:11:26 [selenium.webdriver.remote.remote_connection] DEBUG: Remote response: status=500 | data={"value":{"error":"session not created","message":"session not created: Chrome instance exited. Examine ChromeDriver verbose log to determine the cause.","stacktrace":"#0 0x559153a9616a \u003Cunknown>\n#1 0x559153513c4b \u003Cunknown>\n#2 0x55915354dd1d \u003Cunknown>\n#3 0x5591535496d3 \u003Cunknown>\n#4 0x55915359924c \u003Cunknown>\n#5 0x55915359896c \u003Cunknown>\n#6 0x559153557c42 \u003Cunknown>\n#7 0x5591535588f1 \u003Cunknown>\n#8 0x559153a5ef09 \u003Cunknown>\n#9 0x559153a61e4d \u003Cunknown>\n#10 0x559153a47c51 \u003Cunknown>\n#11 0x559153a62a2b \u003Cunknown>\n#12 0x559153a2ea20 \u003Cunknown>\n#13 0x559153a83a78 \u003Cunknown>\n#14 0x559153a83c49 \u003Cunknown>\n#15 0x559153a954c3 \u003Cunknown>\n#16 0x7f9bd649caa4 \u003Cunknown>\n#17 0x7f9bd6529c6c \u003Cunknown>\n"}} | headers=HTTPHeaderDict({'Content-Length': '791', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-12-11 21:11:26 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2025-12-11 21:11:26 [twisted] CRITICAL: Unhandled error in Deferred:
2025-12-11 21:11:26 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/twisted/internet/defer.py", line 1857, in _inlineCallbacks
    result = context.run(gen.send, result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/crawler.py", line 153, in crawl
    self.spider = self._create_spider(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/crawler.py", line 166, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spiders/__init__.py", line 74, in from_crawler
    spider = cls(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/nebi-scraper/nebi-scraper/nebi_spiders/spiders/containerfritze.py", line 18, in __init__
    self.driver = webdriver.Chrome()
                  ^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/chrome/webdriver.py", line 46, in __init__
    super().__init__(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/chromium/webdriver.py", line 67, in __init__
    super().__init__(command_executor=executor, options=options)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 257, in __init__
    self.start_session(capabilities)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 352, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 432, in execute
    self.error_handler.check_response(response)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.SessionNotCreatedException: Message: session not created: Chrome instance exited. Examine ChromeDriver verbose log to determine the cause.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#sessionnotcreatedexception
Stacktrace:
#0 0x559153a9616a <unknown>
#1 0x559153513c4b <unknown>
#2 0x55915354dd1d <unknown>
#3 0x5591535496d3 <unknown>
#4 0x55915359924c <unknown>
#5 0x55915359896c <unknown>
#6 0x559153557c42 <unknown>
#7 0x5591535588f1 <unknown>
#8 0x559153a5ef09 <unknown>
#9 0x559153a61e4d <unknown>
#10 0x559153a47c51 <unknown>
#11 0x559153a62a2b <unknown>
#12 0x559153a2ea20 <unknown>
#13 0x559153a83a78 <unknown>
#14 0x559153a83c49 <unknown>
#15 0x559153a954c3 <unknown>
#16 0x7f9bd649caa4 <unknown>
#17 0x7f9bd6529c6c <unknown>

2025-12-11 22:54:17 [scrapy.utils.log] INFO: Scrapy 2.13.4 started (bot: nebi_spiders)
2025-12-11 22:54:17 [scrapy.utils.log] INFO: Versions:
{'lxml': '6.0.2',
 'libxml2': '2.14.6',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '25.5.0',
 'Python': '3.11.14 (main, Oct 10 2025, 01:03:14) [GCC 13.3.0]',
 'pyOpenSSL': '25.3.0 (OpenSSL 3.5.4 30 Sep 2025)',
 'cryptography': '46.0.3',
 'Platform': 'Linux-6.11.0-1018-azure-x86_64-with-glibc2.39'}
2025-12-11 22:54:17 [selenium.webdriver.common.selenium_manager] DEBUG: Selenium Manager binary found at: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager
2025-12-11 22:54:17 [selenium.webdriver.common.selenium_manager] DEBUG: Executing process: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome --language-binding python --output json
2025-12-11 22:54:17 [selenium.webdriver.common.selenium_manager] DEBUG: Driver path: /usr/bin/chromedriver
2025-12-11 22:54:17 [selenium.webdriver.common.selenium_manager] DEBUG: Browser path: /usr/bin/google-chrome
2025-12-11 22:54:17 [selenium.webdriver.common.service] DEBUG: Started executable: `/usr/bin/chromedriver` in a child process with pid: 3768 using 0 to output -3
2025-12-11 22:54:17 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://localhost:56087/session {'capabilities': {'firstMatch': [{}], 'alwaysMatch': {'browserName': 'chrome', 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>, 'browserVersion': None, 'goog:chromeOptions': {'extensions': [], 'binary': '/usr/bin/google-chrome', 'args': ['--headless', '--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu', '--window-size=1920,1080']}}}}
2025-12-11 22:54:17 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:56087
2025-12-11 22:54:18 [urllib3.connectionpool] DEBUG: http://localhost:56087 "POST /session HTTP/1.1" 200 0
2025-12-11 22:54:18 [selenium.webdriver.remote.remote_connection] DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"143.0.7499.40","chrome":{"chromedriverVersion":"143.0.7499.40 (c23ff452476d1b6322d73b9b629420ef119d0388-refs/branch-heads/7499@{#2388})","userDataDir":"/tmp/.org.chromium.Chromium.scoped_dir.8yMnpW"},"fedcm:accounts":true,"goog:chromeOptions":{"debuggerAddress":"localhost:39827"},"goog:processID":3778,"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"linux","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:extension:minPinLength":true,"webauthn:extension:prf":true,"webauthn:virtualAuthenticators":true},"sessionId":"a5d5fd4b7d538d578241c172387e412b"}} | headers=HTTPHeaderDict({'Content-Length': '881', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-12-11 22:54:18 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2025-12-11 22:54:18 [scrapy.addons] INFO: Enabled addons:
[]
2025-12-11 22:54:18 [asyncio] DEBUG: Using selector: EpollSelector
2025-12-11 22:54:18 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-12-11 22:54:18 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop
2025-12-11 22:54:18 [scrapy.extensions.telnet] INFO: Telnet Password: 697482363bfe1ef8
2025-12-11 22:54:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-12-11 22:54:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'nebi_spiders',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 0.5,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'logs/containerfritze.txt',
 'NEWSPIDER_MODULE': 'nebi_spiders.spiders',
 'SPIDER_MODULES': ['nebi_spiders.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, '
               'like Gecko) Chrome/141.0.0.0 Safari/537.36'}
2025-12-11 22:54:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-12-11 22:54:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-12-11 22:54:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-12-11 22:54:18 [scrapy.core.engine] INFO: Spider opened
2025-12-11 22:54:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-12-11 22:54:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-12-11 22:54:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://containerfritze.de/> (referer: None)
2025-12-11 22:55:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://containerfritze.de/> (referer: None)
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/defer.py", line 343, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 369, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 369, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 167, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 58, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 167, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 58, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 167, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 58, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 167, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 59, in process_spider_output
    yield from super().process_spider_output(response, result, spider)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 58, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 167, in process_sync
    yield from iterable
  File "/home/runner/work/nebi-scraper/nebi-scraper/nebi_spiders/spiders/containerfritze.py", line 81, in parse
    self.driver.find_elements('xpath', '//ul[@data-attribute="attribute_pa_groesse"]/li/a')[container_num].click()
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 114, in click
    self._execute(Command.CLICK_ELEMENT)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 508, in _execute
    return self._parent.execute(command, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 432, in execute
    self.error_handler.check_response(response)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element <a href="javascript: void(0);" data-attribute-value="5-m%c2%b3-absetzcontainer" data-attribute-value-name="5 mÂ³ Absetzcontainer" class="iconic-was-swatch iconic-was-swatch--image-swatch">...</a> is not clickable at point (1077, 692). Other element would receive the click: <div data-dce-background-overlay-image-url="https://shop.containerfritze.de/wp-content/uploads/2024/03/Containerfritze_Startseite-1-1536x733.jpeg" class="elementor-element elementor-element-0aa24a1 e-flex e-con-boxed e-con e-parent" data-id="0aa24a1" data-element_type="container" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">...</div>
  (Session info: chrome=143.0.7499.40); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#elementclickinterceptedexception
Stacktrace:
#0 0x559fd51c616a <unknown>
#1 0x559fd4c43c4b <unknown>
#2 0x559fd4c9c084 <unknown>
#3 0x559fd4c99f48 <unknown>
#4 0x559fd4c9766a <unknown>
#5 0x559fd4c967b9 <unknown>
#6 0x559fd4c89ad2 <unknown>
#7 0x559fd4c89477 <unknown>
#8 0x559fd4ce0bee <unknown>
#9 0x559fd4c87c42 <unknown>
#10 0x559fd4c888f1 <unknown>
#11 0x559fd518ef09 <unknown>
#12 0x559fd5191e4d <unknown>
#13 0x559fd5177c51 <unknown>
#14 0x559fd5192a2b <unknown>
#15 0x559fd515ea20 <unknown>
#16 0x559fd51b3a78 <unknown>
#17 0x559fd51b3c49 <unknown>
#18 0x559fd51c54c3 <unknown>
#19 0x7f1e5009caa4 <unknown>
#20 0x7f1e50129c6c <unknown>

2025-12-11 22:55:29 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-12-11 22:55:29 [scrapy.core.engine] INFO: Closing spider (finished)
2025-12-11 22:55:29 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/containerfritze.json
2025-12-11 22:55:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 285,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 110752,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 71.337733,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 12, 11, 22, 55, 29, 739764, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 884514,
 'httpcompression/response_count': 1,
 'items_per_minute': 0.0,
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 94920704,
 'memusage/startup': 74428416,
 'response_received_count': 1,
 'responses_per_minute': 0.8450704225352113,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ElementClickInterceptedException': 1,
 'spider_exceptions/count': 1,
 'start_time': datetime.datetime(2025, 12, 11, 22, 54, 18, 402031, tzinfo=datetime.timezone.utc)}
2025-12-11 22:55:29 [scrapy.core.engine] INFO: Spider closed (finished)
